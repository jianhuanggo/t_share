from typing import Optional
import genstudiopy
from genstudiopy.langchain_plugin import llms

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm



# def poc():
#     def test_chat_completion():
#         res = genstudiopy.ChatCompletion.create(
#             model="gpt-35-turbo-v0301",
#             messages=[
#                 {"role": "system", "content": "You are a helpful assistant."},
#                 {"role": "user", "content": "Who won the world series in 2020?"},
#                 {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
#                 {"role": "user", "content": "Where was it played?"}
#             ]
#         )
#         return res
#     return test_chat_completion()


@register_deserializable
class GenAIStudio(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        super().__init__(config=config)

    # NOTE: This class does not use langchain. One reason is that `top_p` is not supported.

    # def test_chat_completion():
    #     res = genstudiopy.ChatCompletion.create(
    #         model="gpt-35-turbo-v0301",
    #         messages=[
    #             {"role": "system", "content": "You are a helpful assistant."},
    #             {"role": "user", "content": "Who won the world series in 2020?"},
    #             {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
    #             {"role": "user", "content": "Where was it played?"}
    #         ]
    #     )
    #     return res
    # return test_chat_completion()

    def get_llm_model_answer(self, prompt):
        messages = []
        if self.config.system_prompt:
            messages.append({"role": "system", "content": self.config.system_prompt})
        messages.append({"role": "user", "content": prompt})

        # print(self.config.model)
        # print("!!!!!")
        # exit(0)
        response = genstudiopy.ChatCompletion.create(
            model=self.config.model or "gpt-4-32k",
            messages=messages,
            # temperature=self.config.temperature,
            temperature=0,
            # max_tokens=self.config.max_tokens,
            max_tokens=8000,
            top_p=self.config.top_p,
            stream=self.config.stream,
        )


        # response  = genstudiopy.ChatCompletion.create(
        #     model="gpt-3.5-turbo",
        #     messages=[
        #         {"role": "system", "content": "You are a helpful assistant."},
        #         {"role": "user", "content": "Who won the world series in 2020?"},
        #         {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        #         {"role": "user", "content": "Where was it played?"}
        #     ]
        # )

        if self.config.stream:
            return self._stream_llm_model_response(response)
        else:
            return response["choices"][0]["message"]["content"]

    def _stream_llm_model_response(self, response):
        """
        This is a generator for streaming response from the OpenAI completions API
        """
        for line in response:
            chunk = line["choices"][0].get("delta", {}).get("content", "")
            yield chunk


from typing import Optional

# Define the attributes and methods of the class as a dictionary
class_dict = {
    '__init__': lambda self, config=None: super(GenAIStudio, self).__init__(config=config),
    'get_llm_model_answer': lambda self, prompt: self._get_llm_model_answer(prompt),
    '_stream_llm_model_response': lambda self, response: self._stream_llm_model_response(response)
}

# Create the GenAIStudio class dynamically using the type() function
GenAIStudio = type('GenAIStudio', (BaseLlm,), class_dict)

# Add type hints to the methods (optional but recommended)
def init(self, config: Optional[BaseLlmConfig] = None) -> None:
    super(GenAIStudio, self).__init__(config=config)

def get_llm_model_answer(self, prompt: str) -> str:
    messages = []
    if self.config.system_prompt:
        messages.append({"role": "system", "content": self.config.system_prompt})
    messages.append({"role": "user", "content": prompt})

    response = genstudiopy.ChatCompletion.create(
        model=self.config.model or "gpt-4-32k",
        messages=messages,
        temperature=0,
        max_tokens=8000,
        top_p=self.config.top_p,
        stream=self.config.stream,
    )

    if self.config.stream:
        return self._stream_llm_model_response(response)
    else:
        return response["choices"][0]["message"]["content"]

def stream_llm_model_response(self, response) -> str:
    for line in response:
        chunk = line["choices"][0].get("delta", {}).get("content", "")
        yield chunk

# Assign the type-hinted methods to the dynamically created class
GenAIStudio.__init__ = init
GenAIStudio.get_llm_model_answer = get_llm_model_answer
GenAIStudio.stream_llm_model_response = stream_llm_model_response
